# -*- coding: utf-8 -*-
"""Swin transformer of Bản sao của EBHI-SEG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hA0sByBXYZSSgwfU_wZUUTNEYq1uX9GN
"""

# Mount the drive into google colab

import warnings
from argparse import ArgumentParser
from pathlib import Path
from imgaug import augmenters as iaa
import timm
warnings.filterwarnings("ignore")
import numpy as np
import pytorch_lightning as pl
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
from torch.optim import SGD, Adam
from torch.utils.data import DataLoader
from torchmetrics import Accuracy
from torchvision import transforms
from torchvision.datasets import ImageFolder

IMAGE_SIZE = 224
models_type = {
    'resnet18': 'resnet18.a1_in1k',
    'resnet34': 'resnet34.a1_in1k',
    'resnet50': 'resnet50.a1_in1k',
    'swinv2_t_w8_256': 'swinv2_tiny_window8_256.ms_in1k',
    'swinv2_t_w16_256': 'swinv2_tiny_window16_256.ms_in1k',
    'swinv2_s_w8_256': 'swinv2_small_window8_256.ms_in1k',
    'swinv2_s_w16_256': 'swinv2_small_window16_256.ms_in1k',
    }
optimizers = {"adam": Adam, "sgd": SGD}


# Here we define a new class to turn the ResNet model that we want to use as a feature extractor
# into a pytorch-lightning module so that we can take advantage of lightning's Trainer object.
# We aim to make it a little more general by allowing users to define the number of prediction classes.
class Classifier(pl.LightningModule):
    def __init__(
        self,
        num_classes,
        model_name,
        train_path,
        val_path,
        test_path=None,
        optimizer="adam",
        lr=1e-4,
        batch_size=16,
        transfer=True,
        tune_fc_only=True,
    ):
        super().__init__()

        self.num_classes = num_classes
        self.train_path = train_path
        self.val_path = val_path
        self.test_path = test_path
        self.lr = lr
        self.batch_size = batch_size
        self.layers = 2
        self.optimizer = optimizers[optimizer]
        # instantiate loss criterion
        self.loss_fn = (
            nn.BCEWithLogitsLoss() if num_classes == 1 else nn.CrossEntropyLoss()
        )
        # create accuracy metric
        self.acc = Accuracy(
            task="binary" if num_classes == 1 else "multiclass", num_classes=num_classes
        )
        self.model_name = model_name
        self.image_size = 256 if '256' in self.model_name else 224 
        print(self.image_size) 
        # Using a pretrained ResNet backbone
        #self.model = self.swin_models[swin_version](pretrained=transfer)
        self.model = timm.create_model(model_name=models_type[model_name], pretrained=transfer)#, in_chans=3)
        # Replace old FC layer with Identity so we can train our own
        if 'swin' in self.model_name:
            linear_size = self.model.head.in_features
            self.model.head.fc = nn.Linear(in_features=linear_size, out_features=6, bias=True)
            print(self.model)

        else:
            #x = torch.randn(1, 3, 224, 224)
            #with torch.no_grad():
            #    feats = self.model.forward_features(x)
        #    linear_size = feats.shape[1]  # torch.Size([1, 2048])
                        #print(self.model)                  
             self.model.reset_classifier(num_classes=num_classes)  # Just changes output size, keeps it linear
        # replace final layer for fine tuning
              #self.model = nn.Sequential(*list(self.model.children())[:-3])
        # nn.Linear(linear_size, num_classes)
        #print(self.model)
        #if tune_fc_only:  # option to only tune the fully-connected layers
        #    for child in list(self.resnet_model.children())[:-1]:
        #        for param in child.parameters():
        #            param.requires_grad = False
        data_config = timm.data.resolve_model_data_config(self.model)
        self.transforms = timm.data.create_transform(**data_config, is_training=False)
        #self.finetune()

    def finetune(self):

        for param in list(self.model.children())[:-1]:
            for p in param.parameters():
                p.requires_grad = False
    
        for param in list(self.model.layers._modules['3'].blocks):
            for p in param.parameters():
                p.requires_grad = True

        if self.layers != 0:
            for param in list(self.model.layers._modules['2'].blocks)[-self.layers:]:
                for p in param.parameters():
                    p.requires_grad = True


    def forward(self, X):
        return self.model(X)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(
            self.model.parameters(), 
            lr = self.lr, 
            weight_decay = 1e-4)
        
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, 
            mode = 'max', 
            factor = 0.2, 
            patience = 50, 
            verbose = True)

        return {
           'optimizer': optimizer,
           'lr_scheduler': scheduler,
           'monitor': 'val_acc' 
       }
    
    #return  
    #return self.optimizer(self.parameters(), lr=self.lr)

    def _step(self, batch):
        x, y = batch
        preds = self(x)
        #print(preds.shape, y.shape)
        #print(preds.unsqueeze(0), preds.unsqueeze(0).shape)
        loss = self.loss_fn(preds, y)
        acc = self.acc(preds, y)
        return loss, acc

    def _dataloader(self, data_path, shuffle=False):
        img_size = self.image_size
        # values here are specific to pneumonia dataset and should be updated for custom data
        tramsform = None
        if "train" in data_path:
          transform = transforms.Compose([
                #np.asarray,
                #    iaa.Sequential([
                #    iaa.Resize({"height": img_size, "width": img_size})
                #    ]).augment_image,
                #np.copy,
               
                transforms.ToTensor(),
                transforms.Resize((img_size, img_size)),
                transforms.Normalize(
                    mean = [0.485, 0.456, 0.406],
                    std = [0.229, 0.224, 0.225])]
          )
        else:
          transform = transforms.Compose([
                #np.asarray,
                #  iaa.Sequential([
                #    iaa.Resize({"height": img_size, "width": img_size})
                #  ]).augment_image,
                #np.copy,

                transforms.ToTensor(),
                transforms.Resize((img_size, img_size)),
                transforms.Normalize(
                    mean = [0.485, 0.456, 0.406],
                    std = [0.229, 0.224, 0.225])
            ]
          )

        img_folder = ImageFolder(data_path, transform=transform)

        return DataLoader(img_folder, batch_size=self.batch_size, shuffle=shuffle, num_workers=32, pin_memory=True)

    def train_dataloader(self):
        return self._dataloader(self.train_path, shuffle=True)

    def training_step(self, batch, batch_idx):
        loss, acc = self._step(batch)
        # perform logging
        self.log(
            "train_loss", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True
        )
        self.log(
            "train_acc", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True
        )
        return loss

    def val_dataloader(self):
        return self._dataloader(self.val_path)

    def validation_step(self, batch, batch_idx):
        loss, acc = self._step(batch)
        # perform logging
        self.log("val_loss", loss, on_epoch=True, prog_bar=False, logger=True)
        self.log("val_acc", acc, on_epoch=True, prog_bar=True, logger=True)

    def test_dataloader(self):
        return self._dataloader(self.test_path)

    def test_step(self, batch, batch_idx):
        loss, acc = self._step(batch)
        # perform logging
        self.log("test_loss", loss, on_step=True, prog_bar=True, logger=True)
        self.log("test_acc", acc, on_step=True, prog_bar=True, logger=True)

def main( 
    model_name, 
    num_classes=6,
    train_path='./new_train',
    val_path='./val',
    test_path='./test',
    optimizer='adam',
    lr=1e-3,
    batch_size=32,
    transfer=True,
    tune_fc_only=False,
    num_epochs = 30,
    save_path = '.',
    mixed_precision = False):


    # # Instantiate Model
    model = Classifier(
        num_classes=num_classes,
        model_name=model_name,
        train_path=train_path,
        val_path=val_path,
        test_path=test_path,
        optimizer=optimizer,
        lr=lr,
        batch_size=batch_size,
        transfer=transfer,
        tune_fc_only=tune_fc_only,
    )
    print(model) 
    save_path = save_path if save_path is not None else "./models"
    checkpoint_callback = pl.callbacks.ModelCheckpoint(
        dirpath=save_path,
        filename=model_name + "-{epoch}-{val_loss:.2f}-{val_acc:0.2f}",
        monitor="val_loss",
        save_top_k=3,
        mode="min",
        save_last=True,
    )

    stopping_callback=pl.callbacks.EarlyStopping(monitor="val_loss", mode="min", patience=40)
    # Instantiate lightning trainer and train model
    trainer_args = {
        "accelerator": "gpu",
        "devices": [0],
        "max_epochs": num_epochs,
        "callbacks": [checkpoint_callback, stopping_callback],
        "precision": 16 if mixed_precision else 32,
    }
    trainer = pl.Trainer(**trainer_args, )

    trainer.fit(model)

    #if test_path:
    #    trainer.test(model)
    # Save trained model weights
    #torch.save(trainer.model.resnet_model.state_dict(), save_path + "/trained_model.pt")

    return model

#
#

# resnet 30 epoch
model = main('swinv2_s_w16_256', num_epochs=100, batch_size=32)
